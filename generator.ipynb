{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import distutils\n",
    "import numpy as np\n",
    "from pickle import load \n",
    "from pickle import dump\n",
    "from PIL import Image \n",
    "import os\n",
    "import string\n",
    "import tensorflow as tf \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_path(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def map_image_to_captions(filename):\n",
    "    file  = load_file_path(filename)\n",
    "    lines  = file.strip().splitlines()\n",
    "    image_to_caption = {}\n",
    "    for line in lines:\n",
    "        img, caption = line.split(',', 1)\n",
    "        img = img.strip()\n",
    "        caption = caption.strip()\n",
    "\n",
    "        if img not in image_to_caption:\n",
    "            image_to_caption[img] = [caption]\n",
    "        else:\n",
    "            image_to_caption[img].append(caption)\n",
    "    return image_to_caption\n",
    "\n",
    "def clean_captions(image_to_caption):\n",
    "    translation_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for img, caps in image_to_caption.items():\n",
    "        for i in range(len(caps)):\n",
    "            img_caption = caps[i].replace(\"-\", \" \")\n",
    "            tokens = img_caption.split()\n",
    "            cleaned_words = [\n",
    "                word.lower().translate(translation_table) for word in tokens\n",
    "                if len(word) > 1 and word.isalpha()\n",
    "            ]\n",
    "            caps[i] = ' '.join(cleaned_words)\n",
    "    return image_to_caption\n",
    "\n",
    "\n",
    "def create_vocab(image_to_caption):\n",
    "    vocab = set()\n",
    "    for img, caps in image_to_caption.items():\n",
    "        for caption in caps:\n",
    "            vocab.update(caption.split()) \n",
    "\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def save_img_to_captions(image_to_caption, filename_to_save):\n",
    "    lines = []\n",
    "    for img, caps in image_to_caption.items():\n",
    "        for caption in caps:\n",
    "            lines.append(img + '\\t' + caption)\n",
    "\n",
    "    data = \"\\n\".join(lines) \n",
    "    with open(filename_to_save, \"w\") as file:\n",
    "        file.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dictionary =  8092\n",
      "length of vocabulary = 8405\n"
     ]
    }
   ],
   "source": [
    "dataset_text = r'C:\\Users\\user\\Desktop\\ML\\Image_Caption_Generator\\flickr8k\\captions.txt'\n",
    "\n",
    "image_to_caption = map_image_to_captions(dataset_text)\n",
    "print(\"length of dictionary = \" ,len(image_to_caption))\n",
    "\n",
    "clean_texts = clean_captions(image_to_caption)\n",
    "\n",
    "vocabulary = create_vocab(clean_texts)\n",
    "print('length of vocabulary =', len(vocabulary))\n",
    "\n",
    "save_img_to_captions(clean_texts, 'img_caption.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception #to get pre-trained model Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "# from keras.preprocessing.text import Tokenizer #for text tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dropout, Reshape, concatenate, Bidirectional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tqdm import tqdm_notebook as tqdm #to check loop progress\n",
    "# tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(include_top=False, pooling = 'avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(directory):\n",
    "    features = {}\n",
    "    for pic in tqdm(os.listdir(directory)):\n",
    "        file_path = os.path.join(directory,pic)\n",
    "\n",
    "        img = Image.open(file_path)\n",
    "        img = img.resize((299,299))\n",
    "        img_array = np.expand_dims(np.array(img) / 127.5 - 1.0, axis=0)\n",
    "        \n",
    "        features_vector = model.predict(img_array, verbose=0)\n",
    "        features[pic] = features_vector\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = r'C:\\Users\\user\\Desktop\\ML\\Image_Caption_Generator\\flickr8k\\Images'\n",
    "features = extract_features(dataset_images)\n",
    "\n",
    "with open(\"features.p\", \"wb\") as file:\n",
    "    dump(features,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"features.p\", \"rb\") as file:\n",
    "    img_features = load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image                                            caption\n",
      "0  1000268201_693b08cb0e.jpg  child in pink dress is climbing up set of stai...\n",
      "1  1000268201_693b08cb0e.jpg                    girl going into wooden building\n",
      "2  1000268201_693b08cb0e.jpg         little girl climbing into wooden playhouse\n",
      "3  1000268201_693b08cb0e.jpg   little girl climbing the stairs to her playhouse\n",
      "4  1000268201_693b08cb0e.jpg  little girl in pink dress going into wooden cabin\n",
      "5  1001773457_577c3a7d70.jpg             black dog and spotted dog are fighting\n"
     ]
    }
   ],
   "source": [
    "df_tok = pd.read_csv(r'C:\\Users\\user\\Desktop\\ML\\Image_Caption_Generator\\img_caption.txt', delimiter='\\t')\n",
    "print(df_tok.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['child in pink dress is climbing up set of stairs in an entry way', 'girl going into wooden building', 'little girl climbing into wooden playhouse', 'little girl climbing the stairs to her playhouse', 'little girl in pink dress going into wooden cabin']\n"
     ]
    }
   ],
   "source": [
    "captions_on_tok =df_tok['caption'].tolist() \n",
    "captions_on_tok = [caption for caption in captions_on_tok if isinstance(caption,str)]\n",
    "print(captions_on_tok[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 310, 61, 191, 114]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(captions_on_tok)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = max(len(caption.split()) for caption in captions_on_tok)\n",
    "tokenizer.texts_to_sequences([captions_on_tok[1]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df_tok['image'].unique().tolist()\n",
    "nimages = len(images)\n",
    "\n",
    "split_index = round(0.85*nimages)\n",
    "train_images = images[:split_index]\n",
    "val_images = images[split_index:]\n",
    "\n",
    "train = df_tok[df_tok['image'].isin(train_images)]\n",
    "test = df_tok[df_tok['image'].isin(val_images)]\n",
    "\n",
    "train.reset_index(inplace=True,drop=True)\n",
    "test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_generator(df, X_col, y_col, batch_size, directory, tokenizer, \n",
    "                          vocab_size, max_length, features, shuffle=True):\n",
    "    data = df.copy()\n",
    "    n = len(data)\n",
    "    indices = np.arange(n)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    # Generator loop\n",
    "    while True:\n",
    "        for start in range(0, n, batch_size):\n",
    "            end = min(start + batch_size, n)\n",
    "            batch_indices = indices[start:end]\n",
    "            batch = data.iloc[batch_indices]\n",
    "            \n",
    "            X1, X2, y = [], [], []\n",
    "\n",
    "            # Loop through each image in the batch\n",
    "            for _, row in batch.iterrows():\n",
    "                image_id = row[X_col]\n",
    "                feature = features[image_id][0]\n",
    "                \n",
    "                # Extract captions and process sequences\n",
    "                captions = row[y_col] if isinstance(row[y_col], list) else [row[y_col]]\n",
    "                for caption in captions:\n",
    "                    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "                    \n",
    "                    for i in range(1, len(seq)):\n",
    "                        in_seq, out_seq = seq[:i], seq[i]\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        \n",
    "                        X1.append(feature)\n",
    "                        X2.append(in_seq)\n",
    "                        y.append(out_seq)\n",
    "            \n",
    "            # Convert lists to numpy arrays for model compatibility\n",
    "            X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
    "            \n",
    "            yield (X1, X2), y\n",
    "\n",
    "        # Shuffle data at the end of each epoch\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\ML\\Image_Caption_Generator\\myenv\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\reshape.py:39: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(2048,))\n",
    "input2 = Input(shape=(max_length,))\n",
    "\n",
    "img_features = Dense(256, activation='relu')(input1)\n",
    "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
    "\n",
    "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
    "merged = concatenate([img_features_reshaped,sentence_features],axis=1)\n",
    "sentence_features = LSTM(256)(merged)\n",
    "x = Dropout(0.5)(sentence_features)\n",
    "x = add([x, img_features])\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "caption_generator_model = Model(inputs=[input1,input2], outputs=output)\n",
    "caption_generator_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(caption_generator_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
